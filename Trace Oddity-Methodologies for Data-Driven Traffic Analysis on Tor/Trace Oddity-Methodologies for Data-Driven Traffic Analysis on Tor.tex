\documentclass[a4paper,12pt]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{wrapfig}
\pagestyle{plain}
\usepackage{fancybox}
\usepackage{bm}

\begin{document}

Trace Oddity: Methodologies for Data-Driven Tra$\mathrm{f}1\mathrm{c}$Analysis on$\mathrm{Tor}$ 323

Algorithm 1. Replication and comparison of a $\mathrm{ML}/\mathrm{DL}$ attack on two di fFerent datasets. Here, with$AP$as a target metric and$t$-test for statistical significance.

Input: Datasets$D_{1}$and$D_{2}, \mathrm{ML}/\mathrm{DL}$ attack$A,$

hyperparameter search space$S.$

Output: Dataset with highest mean attack

performance; the di fFerence between means;

statistical significance.

1 Set number of cross-validation folds$K$

2 Set maximum number of attack configurations$C$ 3 Set number of randomized training runs$n$

$4AP_{D_{1}}\Omega\{\}, AP_{D_{2}}\Omega\{\}$

5 for {\it dataset}$d$oe$\{D_{1},D_{2}\}$do

6

Split$d$into$K$equal folds$d_{1,\ldots,K}$

7

for$k$oe$\{$1, $K\}$do

8

Put aside fold$d_{k}$as$test_{k}$

9

Assign the remaining folds to$train_{k}$

10

Sample$tune_{k}$â„¢$train_{k}$

11

$AP_{best}\Omega 0$

12

for$i$oe$\{$1, $C\}$do

13

Sample config$c_{i}$from$S$using the chosen

optimization strategy

14

Split$tune_{k}$into$tune_{train}, tune_{val}$

15

Train$A$with config$c_{i}$on$tune_{train}$

16

Compute$AP$on$tune_{val}$

17

if $AP>AP_{best}$ then

18

$\lfloor c_{k}=c_{i},AP_{best}=AP$

19

repeatn times

20

21

$\lfloor$Train {\it A}withconfig {\it c}$AP_{d^{\Omega}}AP_{d}\mathrm{fi}A${\it P}Changerandom seedCompute {\it A}$P\mathrm{on}test_{k}$on$train_{k}$

22

23

24 Compute$t$-test and$p$-value on$(AP_{D_{1}},AP_{D_{2}})$

25 return {\it argmax}$\overline{(AP}_{D_{1}}, \overline{AP}_{D_{2}}$) $;|\overline{AP}_{D_{1}}-\overline{AP}_{D_{2}}|$;

$p$-{\it value}.

problem complexity. These could be fundamental dates in the protocol itself, such as defenses$\mathrm{aga}$] tra$\mathrm{c}$analysis. A deep neural network tuned on protected tra$\mathrm{c}$is unlikely to possess enough lear capacity to learn the problem on a harder concep\{ defended tra$\mathrm{c}$and thus needs to be retuned and$\mathrm{t}$] retrained. Moreover, there could be changes in the in representation, such as addition or omission of infor1 tion or input preprocessing. Finally, as is the $\mathrm{cas}\in$ our study, the impact of time and changes in the d collection methodology may a ect the data distribut and the overall problem complexity.

Therefore, to properly deploy and compare$\mathrm{D}$

Corr on our two new datasets, we develop an$att${\it replication}{\it algorithm} for evaluating and comparin deep learning or machine learning-based attack {\it on}{\it ferent}{\it datasets}. The general replication algorithm is

picted in Algorithm 1, it is applicable to all the aforementioned cases and trivially extends to more than two datasets. Further on we clarify some of the steps of the algorithm and discuss why they are meant to account for the primary sources of evaluation bias.

Hyperparameter Tuning. There are several important considerations regarding tuning. Firstly, while the attack used for replication (e.g., a state-of-the-art DL approach) is the same for both datasets, it has to be re-tuned for each given dataset separately for fair comparison. This is to ensure that each dataset is assessed with its own best model configuration (architecture and hyperparameters). Not only because that would be the procedure followed by a real-world adversary, but also because with the best configuration, we are able to more accurately assess the expressive power of the dataset. Secondly, we need to consider resource constraints,

as the algorithm involves many intensive iterations through the datasets. There is a trade-o between on one hand a thorough evaluation that addresses possible biases, and on the other hand staying within reasonable computational time. In terms of tuning, the amount of required resources is mainly controlled by the size of a search space$S$, the maximum number of evaluated attack configurations$C$, and the chosen size of a subset sampled from the training data for tuning (step 10), which could be a smaller fraction or a full training set. Nevertheless, it is important to use a consistent op-

timization strategy that determines which hyperparameter configuration is evaluated at each iteration (step 13). The common options are Grid search (exhaustive search), Random search and Bayesian optimization (probabilistic search). We use Bayesian optimization and elect to start from a small model and gradually increase its complexity. This approach is consistent with the heuristic in Bayesian statistics of preferring simpler models when performance is similar, as they are more likely to represent the underlying distribution of data more accurately [17].

Cross-Validation. To counter selection bias which could arise from evaluation on one fixed data split, the algorithm deploys a procedure similar to the classical $k$-fold cross-validation: di erent portions of the dataset are used to train, validate and test the model on di erent iterations [26]. This way we can assess if the results generalize well, i.e., the attack performs better on one of the datasets over most or even all of the folds. This analysis may increase the confidence in the outcome, or on opposite may show that the result is inconclusive, when the behavior is not consistent across the$K$folds.
\end{document}
